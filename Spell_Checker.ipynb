{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell Checker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdealijaroli/nlp-spell-checker/blob/main/Spell_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRzMZs6q9ZsQ"
      },
      "source": [
        "# Natural Language Processing Project - Spell Checker\n",
        "\n",
        "\n",
        "#### Trinav Rattan (19BCE0493)\n",
        "#### Pallavi Mishra(19BCB0119)\n",
        "#### Abdeali Jaroli(19BCE0190)\n",
        "#### Ayushman Biswari(19BCE0214)\n",
        "#### Naveen Murali(19BDS0157)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4aO1aoiygFC"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WXOIuMsJ02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1effb6-88f3-4280-fb38-d0dece96eeef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poVYqglKZgxN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZf-nAFnnNnq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3tpzaBcUZqQ"
      },
      "source": [
        "# Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI4j-7xv9Zsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d179730-eb8f-4f63-a76c-4be8b46ddb4b"
      },
      "source": [
        "# Importing all required libraries for this task.\n",
        "import nltk\n",
        "import keras\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from itertools import chain\n",
        "import json\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pws2nbD3UePI"
      },
      "source": [
        "# Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjq3kxB9Zsb"
      },
      "source": [
        "def parsing(sent):  \n",
        "    \"\"\"Parsing the sentence to corrected and original and storing in the dictionary.\"\"\"\n",
        "    loriginal = []\n",
        "    lcorrected = []\n",
        "    indexes = []\n",
        "    cnt = 0\n",
        "    \n",
        "    for i in sent:\n",
        "        if '|' in i:\n",
        "            # Splitting the sentence on '|'\n",
        "            str1 = i.split('|')\n",
        "            # Previous word to '|' is storing in loriginal list.\n",
        "            loriginal.append(str1[0])\n",
        "            # Next word to '|' is storing in lcorrected list.\n",
        "            lcorrected.append(str1[1])\n",
        "            #Noting down the index of error.\n",
        "            indexes.append(cnt)\n",
        "        \n",
        "        else:\n",
        "            # If there is no '|' in sentence, sentence is stored in loriginal and lcorrected as it is.\n",
        "            loriginal.append(i)\n",
        "            lcorrected.append(i)\n",
        "        cnt = cnt+1\n",
        "        \n",
        "    #Loading to loriginal, lcorrected and index list to dictionary.      \n",
        "    dictionary = {'original': loriginal, 'corrected': lcorrected, 'indexes': indexes}\n",
        "    \n",
        "    return dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__NzxCNkU3Vn"
      },
      "source": [
        "# Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gomWgd_UxJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0625267-f692-44bc-d519-09975dd15d1b"
      },
      "source": [
        "def preprocessing():\n",
        "    \"\"\"Loading the data from 'holbrook.txt' and passing to parsing function to get parssed sentences. \n",
        "    Returning the whole dictionary as data.\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    # Reading the txt file\n",
        "    text_file = open(\"holbrook.txt\")\n",
        "    lines = []\n",
        "    for i in text_file:\n",
        "        lines.append(i.strip())\n",
        "    \n",
        "    # Word tokenizing the sentences\n",
        "    sentences = [nltk.word_tokenize(sent) for sent in lines]\n",
        "    \n",
        "    # Calling a parse function to get corrected, original sentences.\n",
        "    for sent in sentences:\n",
        "        data.append(parsing(sent))\n",
        "    \n",
        "    return data\n",
        "\n",
        "#Calling preprocessing function\n",
        "data = preprocessing()\n",
        "\n",
        "# Testing\n",
        "print(data[2])\n",
        "assert(data[2] == {\n",
        " 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
        " 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
        " 'indexes': [9]\n",
        "})\n",
        "\n",
        "# Splitting the data to test 100 lines and remaining training lines\n",
        "test = data[:100]\n",
        "train = data[100:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idwpn9CmqPRY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xS69M7qQuK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzyo3ri8U8Jf"
      },
      "source": [
        "#Splitting into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busMPc-f9Zse"
      },
      "source": [
        "# Splitting the data to test - first 100 lines and remaining training lines\n",
        "def test_train_split():\n",
        "    \"\"\"Splitting the data to test - first 100 lines and remaining training lines.\"\"\"\n",
        "    test = data[:100]\n",
        "    train = data[100:]\n",
        "    \n",
        "    # Separating the train original, test original, test corrected and train corrected from dictionary to list.\n",
        "    train_corrected = [elem['corrected'] for elem in train]\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    test_corrected = [elem['corrected'] for elem in test]\n",
        "    test_original = [elem['original'] for elem in test]\n",
        "    \n",
        "    # Removing all special characters from the list.\n",
        "    test_original = [tokenizer.tokenize(\" \".join(elem)) for elem in test_original]\n",
        "    test_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in test_corrected]\n",
        "    train_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in train_corrected]\n",
        "    \n",
        "    return test_corrected, test_original, train_corrected\n",
        "\n",
        "# Test and Training data.\n",
        "test_corrected, test_original, train_corrected = test_train_split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPErjp3uVCws"
      },
      "source": [
        "# Importing Edit Distance function from nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gu1Jksw9Zsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e387368e-9c2e-489f-d246-4dd3bc67aa28"
      },
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNCf6Bc8VISP"
      },
      "source": [
        "# Getting candidates for replacing a misspelled word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzqGtOAT9Zsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0285930b-32f9-45da-c122-0dc708ad4d84"
      },
      "source": [
        "def get_candidates(token):\n",
        "    \n",
        "    \"\"\"Get nearest word for a given incorrect word.\"\"\"\n",
        "    doc = []\n",
        "\n",
        "    for i in train_corrected:\n",
        "        doc.append(\" \".join(i))\n",
        "\n",
        "    doc = \" \".join(doc)\n",
        "    doc = nltk.word_tokenize(doc)\n",
        "    unig_freq = nltk.FreqDist(doc)\n",
        "    unique_words = list(unig_freq.keys())\n",
        "\n",
        "    # Calculate distance between two words\n",
        "    s = []\n",
        "    for i in unique_words:\n",
        "        t = edit_distance(i, token)\n",
        "        s.append(t)\n",
        "    \n",
        "    # Store the nearest words in ordered dictionary\n",
        "    dist = dict(zip(unique_words, s))\n",
        "    dist_sorted = dict(sorted(dist.items(), key=lambda x:x[1]))\n",
        "    minimal_dist = list(dist_sorted.values())[0]\n",
        "    \n",
        "    keys_min = list(filter(lambda k: dist_sorted[k] == minimal_dist, dist_sorted.keys()))\n",
        "    \n",
        "    return keys_min\n",
        "\n",
        "print(get_candidates(\"minde\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mine', 'mind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqHOZL_9Zsi"
      },
      "source": [
        "# This is to calculate unigram and bigram probabilities in correct function\n",
        "doc = []\n",
        "\n",
        "for i in train_corrected:\n",
        "    doc.append(\" \".join(i).lower())\n",
        "\n",
        "doc = \" \".join(doc)\n",
        "doc = nltk.word_tokenize(doc)\n",
        "unig_freq = nltk.FreqDist(doc)\n",
        "unique_words = list(unig_freq.keys())\n",
        "\n",
        "cf_biag = nltk.ConditionalFreqDist(nltk.bigrams(doc))\n",
        "cf_biag = nltk.ConditionalProbDist(cf_biag, nltk.MLEProbDist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAodh9rB6diI"
      },
      "source": [
        "VBG = Verb Gerund (V-ing) <br>\n",
        "VBD = Verb Past Tense (V-ed) <br>\n",
        "VBN = Verb Past Participle (V-ed) <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y282WSuzHYt"
      },
      "source": [
        "def tense(suggestion, sentence):    \n",
        "    \"\"\"Tense Detection\"\"\"\n",
        "    tag = dict(nltk.pos_tag(sentence)).values()\n",
        "    past_tense = ['VBN', 'VBD']\n",
        "    conti_tense = ['VBG']\n",
        "    \n",
        "    # If sentence is past tense append ed and check if it is valid word\n",
        "    if any(x in tag for x in past_tense):\n",
        "        sug = []\n",
        "        for a in suggestion:\n",
        "            if a.lower()+'ed' in unique_words:\n",
        "                sug.append(a+'ed')\n",
        "        for aelem in sug:\n",
        "            suggestion.append(aelem)\n",
        "            \n",
        "    # If sentence is continuous tense append ing and check if it is valid word\n",
        "    if any(x in tag for x in conti_tense):\n",
        "        sug = []\n",
        "        for b in suggestion:\n",
        "            if b.lower()+'ing' in unique_words:\n",
        "                sug.append(b+'ing')\n",
        "        for belem in sug:\n",
        "            suggestion.append(belem)\n",
        "        \n",
        "    return suggestion \n",
        "\n",
        "\n",
        "def named_entity(sentence):\n",
        "    \"\"\"Named Entity Detection using nltk.pos_tag and nltk.ne_chunk\"\"\"\n",
        "    l = []\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(sentence)):\n",
        "        # If any named tag like PERSON, ORGANIZATION or GEOLOCATION append to list.\n",
        "        if hasattr(chunk, 'label'):\n",
        "            l.append(' '.join(c[0] for c in chunk))\n",
        "\n",
        "    \n",
        "    if len(l) != 0:\n",
        "        l = \" \".join(l)\n",
        "        l = l.split(\" \")\n",
        "        \n",
        "    return l\n",
        "\n",
        "\n",
        "def word_forms_new(suggest):\n",
        "    \"\"\"Taking different forms of words using derivationally related forms\"\"\"\n",
        "    sug_form = []\n",
        "    for w in suggest:\n",
        "        forms = set()\n",
        "        for i in wn.lemmas(w):\n",
        "            forms.add(i.name())\n",
        "            for j in i.derivationally_related_forms():\n",
        "                forms.add(j.name())\n",
        "        \n",
        "        for a in list(forms):\n",
        "            sug_form.append(a)\n",
        "    \n",
        "    for q in sug_form:\n",
        "        suggest.append(q)\n",
        "    \n",
        "    word_forms = []\n",
        "    [word_forms.append(i) for i in suggest if not i in word_forms]\n",
        "    return word_forms\n",
        "\n",
        "\n",
        "def conditions(corrected, cr_ind):\n",
        "    \"\"\"Common word - Oclock is not detecting. Hence handling manually but not necessary\"\"\"\n",
        "    \n",
        "    if 'oclock' in corrected:\n",
        "        ind = corrected.index('oclock')\n",
        "        corrected = list(map(lambda x: str.replace(x, \"oclock\", \"clock\"), corrected))\n",
        "        corrected.insert(ind, 'o')\n",
        "        return corrected\n",
        "        \n",
        "    return corrected\n",
        "\n",
        "def sentence_sentence_similarity(sentence1):\n",
        "    \"\"\"Sentence - Sentence similarity using sequence matcher\"\"\"\n",
        "    correc = []\n",
        "    for d in train_corrected:\n",
        "        ratio = SequenceMatcher(None, \" \".join(d), \" \".join(sentence1)).ratio()\n",
        "        if ratio > 98:\n",
        "            correc.append(d)\n",
        "    \n",
        "    if len(correc) == 1:\n",
        "        return correc[0]\n",
        "    else:\n",
        "        return []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiDBMFgxCPfW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqGXRroo1fk"
      },
      "source": [
        "def is_stemmed_not_in_dict(i):\n",
        "  stemmer = PorterStemmer()\n",
        "  return stemmer.stem(i) not in wn.words()\n",
        "\n",
        "def is_lemmatized_not_in_dict(i):\n",
        "  return lemmatizer.lemmatize(i.lower()) not in unique_words\n",
        "\n",
        "def is_word_not_in_known_list(i, n_en, sts):\n",
        "  return all([i.lower() not in source for source in [n_en, sts, wn.words()]]) \n",
        "\n",
        "\n",
        "def check_invalid(i, sts, sentence):\n",
        "  n_en = named_entity(sentence)\n",
        "  return all([is_stemmed_not_in_dict(i), is_lemmatized_not_in_dict(i), \n",
        "              is_word_not_in_known_list(i, n_en, sts), not i.isdigit()])\n",
        "\n",
        "\n",
        "def perform_bigram_distr(suggestion, cnt, sentence):\n",
        "  prob = []\n",
        "  # Bigram probabilities\n",
        "  for sug in suggestion:\n",
        "\n",
        "      # Check the misspelled word is first or last word of the sentence\n",
        "      if ((cnt != 0) and (cnt != len(sentence) - 1)):\n",
        "\n",
        "          try:\n",
        "              p1 = cf_biag[sug.lower()].prob(sentence[cnt + 1].lower())\n",
        "              p2 = cf_biag[corrected[len(corrected) - 1].lower()].prob(sug.lower())\n",
        "              p = p1 * p2\n",
        "              prob.append(p)\n",
        "          except:\n",
        "              prob.append(0)\n",
        "\n",
        "      else:\n",
        "          # If mispelled word is last word of a sencence take probaility of previous word\n",
        "          if cnt == len(sentence) - 1:\n",
        "              try:\n",
        "                  p2 = cf_biag[corrected[len(corrected) - 1].lower()].prob(sug.lower())\n",
        "                  prob.append(p2)\n",
        "              except:\n",
        "                  prob.append(0)\n",
        "\n",
        "\n",
        "          elif cnt == 0:\n",
        "              # If mispelled word is first word of a sencence take probaility of next word\n",
        "              try:\n",
        "                  p1 = cf_biag[sug.lower()].prob(sentence[cnt + 1].lower())\n",
        "                  prob.append(p1)\n",
        "              except:\n",
        "                  prob.append(0)\n",
        "\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwEOzxVQVW32"
      },
      "source": [
        "# Main function - correct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmPEEX2jIli"
      },
      "source": [
        "\n",
        "\n",
        "1.   Stemming and Lemmatizaion\n",
        "2.   Sentence Sentence Similarity\n",
        "3. Named Entity Recgn\n",
        "4. Min Edit Distance\n",
        "5. Tense Correction\n",
        "6. Inclusion of alternative word forms\n",
        "7. Bi gram Probability \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZETkvgXki75P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8m1KmFNzPjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1ca36d-61f2-41cd-818b-ae3f92753558"
      },
      "source": [
        "def correct(sentence):\n",
        "    sts = ['oclock']\n",
        "    corrected = []\n",
        "    cnt = 0\n",
        "    indexes = []\n",
        "    #To check stemmed word in dictonary or not\n",
        "    stemmer = PorterStemmer()\n",
        "    status = 0\n",
        "    #This will extract all named entities of a sentence\n",
        "    n_en = named_entity(sentence)\n",
        "    \n",
        "    for i in sentence:\n",
        "\n",
        "        # Check for sentence similarity\n",
        "        corr = sentence_sentence_similarity(i)\n",
        "        if len(corr) == 1:\n",
        "            return corr\n",
        "        # Ignoring digits like page number and lemmatizing the word and check \n",
        "        # if it is present in dictionary and use words.words() for word validation.\n",
        "       \n",
        "        elif check_invalid(i, sts, sentence):\n",
        "            indexes.append(cnt)\n",
        "            if len(get_candidates(i)) > 1:\n",
        "                # Get words forms, tense detection for suggested sentence\n",
        "                suggestion = get_candidates(i)\n",
        "                suggestion = tense(suggestion, sentence)\n",
        "                wd_fms = word_forms_new(suggestion)\n",
        "                suggestion = wd_fms\n",
        "\n",
        "                prob = perform_bigram_distr(suggestion, cnt, sentence)\n",
        "              \n",
        "                if len(suggestion[prob.index(max(prob))]) > 1:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "                else:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "\n",
        "            else:\n",
        "                corrected.append(get_candidates(i)[0])\n",
        "\n",
        "        else:\n",
        "            corrected.append(i)\n",
        "\n",
        "        cnt = cnt+1\n",
        "        # Manula hadling 'Oclock'\n",
        "        corrected = conditions(corrected, indexes)\n",
        "    \n",
        "    fin = sentence_sentence_similarity(corrected)\n",
        "    if len(fin) != 0:\n",
        "        return fin\n",
        "    else:\n",
        "        return corrected\n",
        "\n",
        "\n",
        "\n",
        "correct(['test', 'of', 'goe', 'out', 'some_times'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'of', 'go', 'out', 'sometimes']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kySNjB2VzuNi"
      },
      "source": [
        "# Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txHjwSSUzkeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9e1565-b859-424b-efa5-e0951b99fce2"
      },
      "source": [
        "start_time = time.time()\n",
        "def accuracy(actual_sent, sent_pred):\n",
        "    \"\"\"This is based on word to word accuracy calculation. \n",
        "    Compares each word with the actual word and calculate accuracy\"\"\"\n",
        "    actual = actual_sent\n",
        "    predict = correct(sent_pred)\n",
        "    # If the blank sentence i.e for a blank line predicted\n",
        "    # is also blank take accuracy as 1\n",
        "\n",
        "    if len(actual) == 0 and len(predict)==0:\n",
        "        accuracy = 1.0\n",
        "    else:\n",
        "        # Take all predicted words same as actual word \n",
        "        #and divide by lenght of sentence\n",
        "\n",
        "        accuracy = len(set(predict) & set(actual))/len(set(actual))\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "acc = []\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "\n",
        "print(\"\\nAverage Accuracy of words in each sentence: \", \n",
        "      round(sum(acc)/len(acc)*100, 4), \"%\")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed Time is: \", elapsed_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:26<00:00,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Accuracy of words in each sentence:  89.2142 %\n",
            "Elapsed Time is:  206.9772436618805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOo9y-kgVeE9"
      },
      "source": [
        "# Testing code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1hbx0v3Lct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09dbcdb9-6dc7-4920-8320-e9409110da4e"
      },
      "source": [
        "def test(sent):\n",
        "    print(\"\\nOriginal: \" + sent)\n",
        "    print(\"Corrected: \" + ' '.join(correct(sent.split())))\n",
        " \n",
        "test(\"Ram is goooood\")\n",
        "test(\"Adam is whitr in color\")\n",
        "test(\"look ahedd\")\n",
        "test(\"NLP is my favourite courde\")\n",
        "test(\"I am hapoy\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original: Ram is goooood\n",
            "Corrected: Ram is good\n",
            "\n",
            "Original: Adam is whitr in color\n",
            "Corrected: Adam is white in color\n",
            "\n",
            "Original: look ahedd\n",
            "Corrected: look ahead\n",
            "\n",
            "Original: NLP is my favourite courde\n",
            "Corrected: NLP is my favourite course\n",
            "\n",
            "Original: I am hapoy\n",
            "Corrected: I am happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fehu-zA1b8Te"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}